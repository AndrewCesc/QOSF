{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task \\#2: Encoding and Classifier"
      ],
      "metadata": {
        "id": "N6Hdrro6JUbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and import dependencies"
      ],
      "metadata": {
        "id": "6833Z9CeHJBm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia1G0YBSdUA0",
        "outputId": "c8be3d4b-56ab-4162-d5ec-cf9b723395c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.7/dist-packages (0.21.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from pennylane) (2.6.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pennylane) (1.4.1)\n",
            "Requirement already satisfied: retworkx in /usr/local/lib/python3.7/dist-packages (from pennylane) (0.11.0)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.7/dist-packages (from pennylane) (1.3)\n",
            "Requirement already satisfied: semantic-version==2.6 in /usr/local/lib/python3.7/dist-packages (from pennylane) (2.6.0)\n",
            "Requirement already satisfied: autoray in /usr/local/lib/python3.7/dist-packages (from pennylane) (0.2.5)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pennylane) (0.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pennylane) (1.21.5)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from pennylane) (4.2.4)\n",
            "Requirement already satisfied: pennylane-lightning>=0.21 in /usr/local/lib/python3.7/dist-packages (from pennylane) (0.21.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.7/dist-packages (from pennylane-lightning>=0.21->pennylane) (1.10.2.3)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd->pennylane) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pennylane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "IFS6qJFodykB"
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "from itertools import chain\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "from pennylane.templates.embeddings import AngleEmbedding\n",
        "from pennylane.templates.layers import StronglyEntanglingLayers\n",
        "from pennylane.optimize import GradientDescentOptimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import data"
      ],
      "metadata": {
        "id": "uufd9nttHScu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "train = pd.read_csv('mock_train_set.csv')\n",
        "test = pd.read_csv('mock_test_set.csv')"
      ],
      "metadata": {
        "id": "i5nLc1MOHUyJ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess data\n",
        "\n",
        "- Feature 0 is scaled between [-1,1];\n",
        "- Feature 1 is encoded categorically;\n",
        "- Feature 2 is encoded categorically;\n",
        "- Feature 3 is scaled between [-1,1];"
      ],
      "metadata": {
        "id": "FYEPJ_K1HfHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainX = train[['0','1','2','3']].values\n",
        "testX = test[['0','1','2','3']].values\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaler.fit(trainX[:,0].reshape(-1,1))\n",
        "trainX[:,0] = scaler.transform(trainX[:,0].reshape(-1,1)).squeeze()\n",
        "testX[:,0] = scaler.transform(testX[:,0].reshape(-1,1)).squeeze()\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "le.fit(trainX[:,1].reshape(-1,1))\n",
        "trainX[:,1] = le.transform(trainX[:,1].reshape(-1,1))\n",
        "testX[:,1] = le.transform(testX[:,1].reshape(-1,1))\n",
        "\n",
        "\n",
        "le2 = preprocessing.LabelEncoder()\n",
        "\n",
        "le2.fit(trainX[:,2].reshape(-1,1))\n",
        "trainX[:,2] = le2.transform(trainX[:,2].reshape(-1,1))\n",
        "testX[:,2] = le2.transform(testX[:,2].reshape(-1,1))\n",
        "\n",
        "scaler2 = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaler.fit(trainX[:,3].reshape(-1,1))\n",
        "trainX[:,3] = scaler.transform(trainX[:,3].reshape(-1,1)).squeeze()\n",
        "testX[:,3] = scaler.transform(testX[:,3].reshape(-1,1)).squeeze()\n",
        "\n",
        "\n",
        "trainX = Variable(torch.Tensor(trainX))\n",
        "trainY = Variable(torch.Tensor(train['4'].values))\n",
        "\n",
        "\n",
        "testX = Variable(torch.Tensor(testX))\n",
        "testY = Variable(torch.Tensor(test['4'].values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfM4CWf4HeHm",
        "outputId": "2b7f5d18-8b99-4928-82d1-bcf4aa54fd30"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Angle Embedding\n",
        "\n",
        "Encode data into qubits' angles through rotation gates and define three BasicEntanglerLayers: one with RZ, one with RX and one with RY rotations. All of them have CNOTs as entangling gates. **Two** repetitions of each entangling layer are defined.\n",
        "\n",
        "Measure in Pauli-Z basis just the first qubit, and retrieve its expectation value."
      ],
      "metadata": {
        "id": "3_QdUtarI300"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbY9cQ0Od5aN",
        "outputId": "28bdc1b3-f275-466f-bb57-5c472845e3f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight shape: {'weights': (6, 4)}\n"
          ]
        }
      ],
      "source": [
        "n_qubits = 4\n",
        "dev = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\", diff_method='adjoint')\n",
        "def qnode(inputs, weights):\n",
        "    #print('inputs:',inputs,'\\n\\n','weights:',weights)\n",
        "    w = weights.shape[0]//3\n",
        "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
        "    qml.BasicEntanglerLayers(weights[:w], wires=range(n_qubits), rotation=qml.RZ)\n",
        "    qml.BasicEntanglerLayers(weights[w:2*w], wires=range(n_qubits))\n",
        "    qml.BasicEntanglerLayers(weights[2*w:], wires=range(n_qubits), rotation=qml.RY)\n",
        "\n",
        "    return [qml.expval(qml.PauliZ(wires=i)) for i in range(1)] #measure the first qubit only, as reported in...\n",
        "\n",
        "n_layers=2\n",
        "entangling_layers = 3\n",
        "weight_shapes = {\"weights\": (n_layers*entangling_layers, n_qubits)}\n",
        "print('weight shape:',weight_shapes)\n",
        "\n",
        "qlayerAngleEmbedding = qml.qnn.TorchLayer(qnode, weight_shapes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Pytorch interface and define the Net class"
      ],
      "metadata": {
        "id": "lLNJnI5qKATq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "jOeZru1mnP6Q"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.qc = qlayerAngleEmbedding\n",
        "        self.fctest = nn.Linear(4,1) #Useful for classical NN comparison\n",
        "        #self.fc = nn.Linear(2,1)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        #print(x)\n",
        "        x = self.qc(x)\n",
        "        #x = self.fctest(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    \n",
        "    def predict(self, x):\n",
        "        # apply softmax\n",
        "        pred = self.forward(x)\n",
        "#         print(pred)\n",
        "        #ans = torch.argmax(pred).item()\n",
        "        return torch.tensor(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and testing"
      ],
      "metadata": {
        "id": "JyymCeidKObB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YFSWxprd-fV",
        "outputId": "bf7b9720-8c6a-44d8-c31e-0d0f71fe5c95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 0 ~ Loss 0.389044 \t\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [5%]\t Training Loss: 0.6956 Test Loss: 0.5270\n",
            " Epoch 1 ~ Loss 0.310232 \t\tTraining [10%]\t Training Loss: 0.4559 Test Loss: 0.3583\n",
            " Epoch 2 ~ Loss 0.254295 \t\tTraining [15%]\t Training Loss: 0.3458 Test Loss: 0.2757\n",
            " Epoch 3 ~ Loss 0.209780 \t\tTraining [20%]\t Training Loss: 0.2902 Test Loss: 0.2301\n",
            " Epoch 4 ~ Loss 0.178556 \t\tTraining [25%]\t Training Loss: 0.2595 Test Loss: 0.2038\n",
            " Epoch 5 ~ Loss 0.158638 \t\tTraining [30%]\t Training Loss: 0.2417 Test Loss: 0.1881\n",
            " Epoch 6 ~ Loss 0.147348 \t\tTraining [35%]\t Training Loss: 0.2305 Test Loss: 0.1783\n",
            " Epoch 7 ~ Loss 0.142867 \t\tTraining [40%]\t Training Loss: 0.2225 Test Loss: 0.1718\n",
            " Epoch 8 ~ Loss 0.144002 \t\tTraining [45%]\t Training Loss: 0.2161 Test Loss: 0.1669\n",
            " Epoch 9 ~ Loss 0.149583 \t\tTraining [50%]\t Training Loss: 0.2102 Test Loss: 0.1630\n",
            " Epoch 10 ~ Loss 0.158246 \t\tTraining [55%]\t Training Loss: 0.2048 Test Loss: 0.1594\n",
            " Epoch 11 ~ Loss 0.168651 \t\tTraining [60%]\t Training Loss: 0.1996 Test Loss: 0.1560\n",
            " Epoch 12 ~ Loss 0.179923 \t\tTraining [65%]\t Training Loss: 0.1949 Test Loss: 0.1527\n",
            " Epoch 13 ~ Loss 0.191882 \t\tTraining [70%]\t Training Loss: 0.1906 Test Loss: 0.1495\n",
            " Epoch 14 ~ Loss 0.204900 \t\tTraining [75%]\t Training Loss: 0.1867 Test Loss: 0.1461\n",
            " Epoch 15 ~ Loss 0.219495 \t\tTraining [80%]\t Training Loss: 0.1826 Test Loss: 0.1421\n",
            " Epoch 16 ~ Loss 0.235725 \t\tTraining [85%]\t Training Loss: 0.1781 Test Loss: 0.1368\n",
            " Epoch 17 ~ Loss 0.252350 \t\tTraining [90%]\t Training Loss: 0.1725 Test Loss: 0.1299\n",
            " Epoch 18 ~ Loss 0.266647 \t\tTraining [95%]\t Training Loss: 0.1661 Test Loss: 0.1221\n",
            " Epoch 19 ~ Loss 0.276078 \t\tTraining [100%]\t Training Loss: 0.1599 Test Loss: 0.1150\n",
            "\n",
            "Accuracy score on test: 93.33%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
        "import math\n",
        "import torch.optim as optim\n",
        "\n",
        "epochs = 20\n",
        "torch.manual_seed(47)\n",
        "random.seed(47)\n",
        "\n",
        "network = Net()\n",
        "    \n",
        "optimizer = optim.Adam(network.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "network.train()\n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  total_loss = []\n",
        "  for batch_idx in range(len(trainY)):\n",
        "    data, target = trainX[batch_idx], trainY[batch_idx].view(1)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)        \n",
        "    # Forward pass\n",
        "    output = network(data)\n",
        "    #print(output,target)\n",
        "    # Calculating loss\n",
        "    loss = loss_func(output, target)\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_(network.parameters(), 1.0)\n",
        "    # Optimize the weights\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss.append(loss.item())    \n",
        "    print('\\r Epoch %d ~ Loss %f ' % (epoch, loss.item()), end='\\t\\t')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    val_loss = []\n",
        "    targets = []\n",
        "    predictions = []\n",
        "    for batch_idx in range(len(testX)):\n",
        "      data, target = testX[batch_idx], testY[batch_idx].view(1)\n",
        "      output = network(data)\n",
        "      loss = loss_func(output, target)\n",
        "\n",
        "      val_loss.append(loss.item())\n",
        "\n",
        "      targets.append(target)\n",
        "      predictions.append(network.predict(data))\n",
        "\n",
        "  train_loss_list.append(sum(total_loss)/len(total_loss))\n",
        "  val_loss_list.append(sum(val_loss)/len(val_loss))\n",
        "\n",
        "  print('Training [{:.0f}%]\\t Training Loss: {:.4f} Test Loss: {:.4f}'.format(\n",
        "      100. * (epoch + 1) / epochs, train_loss_list[-1], val_loss_list[-1]))\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i]>0.5:\n",
        "    predictions[i]=1\n",
        "  else:\n",
        "    predictions[i]=0\n",
        "    \n",
        "print('\\nAccuracy score on test: %.2f%%' % (accuracy_score(testY,predictions)*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Amplitude Embedding\n",
        "\n",
        "Encode data into qubits' amplitude, so we can use 2 qubits only instead of 4 qubits (previous case). BasicEntanglerLayers: one with RZ, one with RX and one with RY rotations. All of them have CNOTs as entangling gates. **Three** repetitions of each entangling layer are defined.\n",
        "\n",
        "Measure in Pauli-Z basis just the first qubit, and retrieve its expectation value."
      ],
      "metadata": {
        "id": "1PI4yC6XKhtT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKoDOfOccYzw",
        "outputId": "deec4926-7af6-4c5a-92e7-6f8fad7880e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight shape: {'weights': (9, 2)}\n"
          ]
        }
      ],
      "source": [
        "n_qubits = 2\n",
        "dev = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\", diff_method='adjoint')\n",
        "def qnode(inputs, weights):\n",
        "    #print('inputs:',inputs,'\\n\\n','weights:',weights)\n",
        "    w = weights.shape[0]//3\n",
        "    qml.AmplitudeEmbedding(inputs, wires=range(n_qubits), normalize=True)\n",
        "    qml.BasicEntanglerLayers(weights[:w], wires=range(n_qubits), rotation=qml.RZ)\n",
        "    qml.BasicEntanglerLayers(weights[w:2*w], wires=range(n_qubits))\n",
        "    qml.BasicEntanglerLayers(weights[2*w:], wires=range(n_qubits), rotation=qml.RY)\n",
        "    #qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
        "    #qml.BasicEntanglerLayers(weights[8:], wires=range(n_qubits))\n",
        "\n",
        "    return [qml.expval(qml.PauliZ(wires=i)) for i in range(1)] #measure the first qubit only, as reported in...\n",
        "\n",
        "n_layers=3\n",
        "entangling_layers = 3\n",
        "weight_shapes = {\"weights\": (n_layers*entangling_layers, n_qubits)}\n",
        "print('weight shape:',weight_shapes)\n",
        "\n",
        "qlayerAmplitudeEmbedding = qml.qnn.TorchLayer(qnode, weight_shapes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Pytorch interface and define the Net2 class"
      ],
      "metadata": {
        "id": "LqOhs2MjKxBG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "iwfuKTOxcf0H"
      },
      "outputs": [],
      "source": [
        "class Net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2, self).__init__()\n",
        "        \n",
        "        self.qc = qlayerAmplitudeEmbedding\n",
        "        self.fctest = nn.Linear(4,1) #Useful for classical NN comparison\n",
        "        #self.fc = nn.Linear(2,1)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        #print(x)\n",
        "        x = self.qc(x)\n",
        "        #x = self.fctest(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    \n",
        "    def predict(self, x):\n",
        "        # apply softmax\n",
        "        pred = self.forward(x)\n",
        "#         print(pred)\n",
        "        #ans = torch.argmax(pred).item()\n",
        "        return torch.tensor(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and testing"
      ],
      "metadata": {
        "id": "PAfKs0E7KzQ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD13Twk8civm",
        "outputId": "8dd32ba5-6c80-42ad-c7c6-b14363ec037f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 0 ~ Loss 0.753418 \t\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training [5%]\t Training Loss: 0.4044 Test Loss: 0.2959\n",
            " Epoch 1 ~ Loss 0.326499 \t\tTraining [10%]\t Training Loss: 0.2222 Test Loss: 0.1635\n",
            " Epoch 2 ~ Loss 0.187862 \t\tTraining [15%]\t Training Loss: 0.1668 Test Loss: 0.1192\n",
            " Epoch 3 ~ Loss 0.133999 \t\tTraining [20%]\t Training Loss: 0.1500 Test Loss: 0.1034\n",
            " Epoch 4 ~ Loss 0.108292 \t\tTraining [25%]\t Training Loss: 0.1441 Test Loss: 0.0962\n",
            " Epoch 5 ~ Loss 0.094223 \t\tTraining [30%]\t Training Loss: 0.1413 Test Loss: 0.0923\n",
            " Epoch 6 ~ Loss 0.085576 \t\tTraining [35%]\t Training Loss: 0.1396 Test Loss: 0.0898\n",
            " Epoch 7 ~ Loss 0.079653 \t\tTraining [40%]\t Training Loss: 0.1384 Test Loss: 0.0881\n",
            " Epoch 8 ~ Loss 0.075217 \t\tTraining [45%]\t Training Loss: 0.1375 Test Loss: 0.0869\n",
            " Epoch 9 ~ Loss 0.071675 \t\tTraining [50%]\t Training Loss: 0.1368 Test Loss: 0.0859\n",
            " Epoch 10 ~ Loss 0.068727 \t\tTraining [55%]\t Training Loss: 0.1362 Test Loss: 0.0851\n",
            " Epoch 11 ~ Loss 0.066207 \t\tTraining [60%]\t Training Loss: 0.1357 Test Loss: 0.0845\n",
            " Epoch 12 ~ Loss 0.064016 \t\tTraining [65%]\t Training Loss: 0.1352 Test Loss: 0.0839\n",
            " Epoch 13 ~ Loss 0.062090 \t\tTraining [70%]\t Training Loss: 0.1349 Test Loss: 0.0835\n",
            " Epoch 14 ~ Loss 0.060385 \t\tTraining [75%]\t Training Loss: 0.1345 Test Loss: 0.0831\n",
            " Epoch 15 ~ Loss 0.058867 \t\tTraining [80%]\t Training Loss: 0.1343 Test Loss: 0.0828\n",
            " Epoch 16 ~ Loss 0.057511 \t\tTraining [85%]\t Training Loss: 0.1340 Test Loss: 0.0825\n",
            " Epoch 17 ~ Loss 0.056295 \t\tTraining [90%]\t Training Loss: 0.1338 Test Loss: 0.0823\n",
            " Epoch 18 ~ Loss 0.055202 \t\tTraining [95%]\t Training Loss: 0.1337 Test Loss: 0.0821\n",
            " Epoch 19 ~ Loss 0.054218 \t\tTraining [100%]\t Training Loss: 0.1335 Test Loss: 0.0820\n",
            "\n",
            "Accuracy score on test: 93.33%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
        "import math\n",
        "import torch.optim as optim\n",
        "\n",
        "epochs = 20\n",
        "torch.manual_seed(47)\n",
        "random.seed(47)\n",
        "\n",
        "network = Net2()\n",
        "    \n",
        "optimizer = optim.Adam(network.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "network.train()\n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  total_loss = []\n",
        "  for batch_idx in range(len(trainY)):\n",
        "    data, target = trainX[batch_idx], trainY[batch_idx].view(1)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)        \n",
        "    # Forward pass\n",
        "    output = network(data)\n",
        "    #print(output,target)\n",
        "    # Calculating loss\n",
        "    loss = loss_func(output, target)\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_(network.parameters(), 1.0)\n",
        "    # Optimize the weights\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss.append(loss.item())    \n",
        "    print('\\r Epoch %d ~ Loss %f ' % (epoch, loss.item()), end='\\t\\t')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    val_loss = []\n",
        "    targets = []\n",
        "    predictions = []\n",
        "    for batch_idx in range(len(testX)):\n",
        "      data, target = testX[batch_idx], testY[batch_idx].view(1)\n",
        "      output = network(data)\n",
        "      loss = loss_func(output, target)\n",
        "\n",
        "      val_loss.append(loss.item())\n",
        "\n",
        "      targets.append(target)\n",
        "      predictions.append(network.predict(data))\n",
        "\n",
        "  train_loss_list.append(sum(total_loss)/len(total_loss))\n",
        "  val_loss_list.append(sum(val_loss)/len(val_loss))\n",
        "\n",
        "  print('Training [{:.0f}%]\\t Training Loss: {:.4f} Test Loss: {:.4f}'.format(\n",
        "      100. * (epoch + 1) / epochs, train_loss_list[-1], val_loss_list[-1]))\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i]>0.5:\n",
        "    predictions[i]=1\n",
        "  else:\n",
        "    predictions[i]=0\n",
        "    \n",
        "print('\\nAccuracy score on test: %.2f%%' % (accuracy_score(testY,predictions)*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "Angle embedding allows to achieve slightly better performances with respect to amplitude encoding, although it uses as many qubits as the # of features. Amplitude encoding, instead, just uses $log_2(d)$ qubits, where $d$ is the number of features.\n",
        "\n",
        "In both the configurations, rotations around X, Y and Z axes are necessary to increase the expressivity of the circuit and to better approximate the underlying function."
      ],
      "metadata": {
        "id": "--IXZaQlLEoo"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "QOSF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}